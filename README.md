ğŸ›’ Salling Group â€“ Anti Food Waste Data Pipeline
ğŸ“Œ Project Overview

This project demonstrates a complete end-to-end data pipeline for collecting, processing, storing, and analysing Anti Food Waste offers from the Salling Group API.

The solution is designed to run both locally and on a Linux server (Ubuntu / AWS EC2) and focuses on automation, reproducibility, and clean data handling.

Developed as part of an academic examination

Awarded the highest grade (12) for technical implementation and structure

ğŸ¯ Project Purpose

The project aims to:

Automatically collect Anti Food Waste data by postcode

Store structured data in a relational database

Enable analysis of:

product types

time-based patterns

Demonstrate real-world data engineering workflows

âš™ï¸ Key Features

API ingestion using the Salling Group Food Waste API

Parameterised execution via command line arguments

Automated execution using cron

Execution logging for monitoring and debugging

Secure credential handling using environment variables

Separation of:

data collection

processing logic

data storage

ğŸ§° Tech Stack

R â€“ data collection and transformation

MariaDB / MySQL â€“ relational data storage

Ubuntu Linux â€“ server environment

AWS EC2 â€“ cloud hosting

Git & GitHub â€“ version control and deployment

cron â€“ task scheduling and automation

```bash
ğŸ“ Project Structure
Salling-Group/
â”‚
â”œâ”€â”€ Valby test.R           # Main data pipeline script
â”œâ”€â”€ update.log             # Log file from scheduled runs
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ main_df.xlsx           # Example output (local testing)
â”œâ”€â”€ main_clearances.xlsx   # Example output (local testing)
â””â”€â”€ SQL/                   # SQL table definitions (optional)
```

ğŸ”„ How the Pipeline Works

Reads postcode from the command line

Fetches Anti Food Waste data from the API

Cleans and structures store and offer data

Converts timestamps from ISO 8601 to SQL datetime

Writes data to the database:

main_df (stores)

clearance_df (offers)

Logs execution status and timestamps

â–¶ï¸ Running the Script
```bash
Rscript "Valby test.R" 2500

Manual execution with logging
Rscript "Valby test.R" 2500 >> update.log 2>&1

Automated execution (cron â€“ hourly example)
0 * * * * /usr/bin/Rscript /home/ubuntu/git/Salling-Group/Valby\ test.R 2500 >> /home/ubuntu/git/Salling-Group/update.log 2>&1
```

ğŸ—„ï¸ Database Design

main_df

One row per store

Primary key: store_id

clearance_df

One row per offer

Linked to stores via store_id

This structure supports:

efficient SQL queries

downstream analysis in R

scalable data collection

ğŸ” Security & Best Practices

Database credentials handled via Sys.getenv()

No secrets committed to GitHub

Same codebase runs locally and on server

Absolute paths used for cron execution

ğŸš€ Possible Extensions

Support for multiple postcodes per run

Deduplication and historical tracking

Dashboard visualisation (Power BI / Shiny)

Predictive modelling of offer timing

Integration with additional data sources

ğŸ‘¤ Author

Stefan Torp Bondo
